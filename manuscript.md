---
title: "Standardising Peer Review in Paleontology journals"
author: "Jon Tennant"
date: "30 December 2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

# Introduction




“However ill-defined it may be, the peer-review process is still the gold standard that will continue to drive scholarly publication.” (source) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4093306/

I’m not really sure what this means. It sounds like, ‘no-one knows what peer review really is, but we’ll maintain it at the core of research anyway’..
The same for..
“While it is not a perfect process, traditional peer review remains the gold standard for evaluating and selecting quality scientific publications.” (source) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4093306/

We keep repeating this mantra in academia as a sort of shoulder shrug. My feeling on the matter is this:
“How can something exclusive, secretive, and irreproducible be considered to be objective? How can something exclusive, secretive, and irreproducible be considered as a ‘gold standard’ of any sort?” (source) http://blog.scienceopen.com/2017/04/a-new-gold-standard-of-peer-review-is-needed/

Because of this, we have absolutely no idea what goes into the hyperdiverse process of peer review. We know that when something comes out, it is conferred the status and stamp of ‘peer reviewed’. For some bizarrely unscientific reason, we then maintain this as a pseudo-standard for validity and quality. Except we don’t actually have any knowledge of the process that led to this, because that’s how peer review largely works. Academic logic at its finest.
Peer review fails peer review, and its own test of integrity and validation, and is one of the greatest ironies of the academic world.



What we should be thinking about instead is how to inject some science into our primary method of ‘scientific validation’. Like any human endeavor, it will always be at least partially subjective, but that doesn’t mean we can try to eliminate many of the well-documented problems it has (see ‘Problems with peer review‘ here). https://f1000research.com/articles/6-588/v1

I wrote about how peer review in general seems to be lacking any form of standardisation recently, due to its inherent traits of being exclusive, secretive, and irreproducible. We as the scientific community should be thinking more about the management, functionality, and process of peer review. It is our collective responsibility that peer review is held to the same high standards of integrity that we do for research. http://blog.scienceopen.com/2017/04/a-new-gold-standard-of-peer-review-is-needed/

Until we can do this, I don’t think we should keep pretending that peer review is any sort of ‘gold standard’, because we just don’t have any evidence to support this. However, what we don’t want to create is another standard, if some already exist, as that would just be redundant.

Some things that exist already
•	Code of Conduct and best practice guidelines for journal editors (link) https://publicationethics.org/files/Code of Conduct_2.pdf
•	COPE ethical guidelines for peer reviewers (link) https://publicationethics.org/files/Peer review guidelines_0.pdf
•	Peer Reviewers Openness Initiative (link) https://opennessinitiative.org/
•	A guide to peer review, British Ecological Society (link) http://www.britishecologicalsociety.org/wp-content/uploads/2016/04/Guide-to-Peer-Review.pdf
•	Referee guidelines, Institute of Physics (link) http://ioppublishing.org/img/landingPages/guidelines-and-policies/referee-guidelines.html


The COPE guidelines probably come closest to what we have as a standard: “The COPE Ethical Guidelines for Peer Reviewers set out the basic principles and standards to which all peer reviewers should adhere during the peer review process.“
To me, these are mostly sets of principles or guidelines to ‘do a good job’. They don’t quite meet the idea of a ‘standard’, as they don’t define a level of technical quality or attainment that can be used for comparative purposes. Which is useful, say, for examining the utility and functionality of peer review. Guidelines also appear to be a bit too general and open to interpretation at times. The power of a standard comes in its specificity, which doesn’t really work too well for a process as diverse as peer review.

I think the COPE guidelines are great, but that we could use them as the basis for something more. For example, having each item in the guidelines as a checklist for referees. Here, each peer review would have to satisfy a certain ‘quality’ threshold before it is accepted. This would, in my eyes, make it more of a ‘technical standard‘ (i.e., an operational checklist). Ultimately, this would also progress it towards conforming to an ‘open standard‘.

https://en.wikipedia.org/wiki/Technical_standard

https://en.wikipedia.org/wiki/Open_standard

The main issue that arises is that an actual standard might stifle diversity and creativity in peer review. This is certainly not a good thing.

Academics hate rules. A standard or set of guidelines creates a rigid system that research suffers far too much from already. The idea of implementing a standard for something as inherently diverse as peer review has potential desirable outcomes, such as increasing overall quality. But this might come at the cost of becoming too narrow or rigid a process. What we need is for peer review to remain flexible for different communities, but to still perform the basic tasks we hold it to.

Really, the issue lies with pretending that peer review has any sort of standard as a process at all. The quality of peer review is impossible to assess, and therefore we can rarely see whether it has attained a certain threshold or not. It is impossible to quantify, again due to it being a hidden process, which means that we cannot use it for comparative purposes.

So when people say ‘gold standard’ of peer review, I don’t really know what that means. It’s more like holding on to the concept of it as this magical process of validation and gatekeeping, when really we have very little idea if it actually does these effectively. In fact, much of the evidence suggests that peer review fails to even meet the most basic ideals that we hold it to. So practically, I think we need to explicitly stop calling peer review a ‘gold standard’ of any sort. But does this mean it needs to become ‘standardised’?

All of these guidelines and principles represent more bureaucracy in an already bloated bureaucratic academia. Instead, we could and should be teaching students the reasons why peer review is important. Get this right, and you won’t need external guidelines, as they’ll be embedded as a culture anyway.
What’s better: Having to go through a checklist for each peer review, or knowing in advance how to perform a high quality peer review?
So instead of guidelines or standards, what we need is better training in the why of peer review. Imagine this as a couple of very simple examples:
•	Why: High quality research is supposed to be reproducible; 
o	Consequence: Referees request code and data etc. be made available during peer review to test results;
•	Why: Published research is supposed to be valid; 
o	Consequence: Referees make sure conclusions are supported by results;
o	Consequence: Referees make sure analyses have been conducted correctly;
•	Why: Published research is supposed to be high quality; 
o	Consequence: Referees make sure research passes basic tests of integrity;
o	Consequence: Peer review factors into objective assessments of research quality.
These translations of principles to practices shows how teaching students/researchers why can lead to automatically conducting peer review at a higher level. How much better is this than telling students they have to do it as “part of their academic duty”.


Peer review is widely considered to be fundamental in maintaining the rigour and validity of scholarly research. However, the process is often opaque, which can introduce bias into reporting standards for research and impact the overall quality of the published record.

Presently, peer review is a non-standardised process, either across or within disciplines. Training and support is generally lacking, and it is often the case that reviewers, through no fault of their own, are unaware of the critical questions to be asking with respect to research design, methods, reporting, and analysis. Given that there are over 200 journals that publish palaeontology research (Tennant and Lomax, 2019), this can make the process confusing for reviewers, irrespective of their relative expertise. 

The aim of this project is to formulate a clear set of guidelines explicitly for reviewers in palaeontology journals, or multi-disciplinary journals that include palaeontology submissions. Through this, peer review can be more transparent and as objective as possible, representing a form of peer review best practice. It can also help to improve the soundness and reporting standards for paleontology research as a whole.

These guidelines were inspired by Parker et al. 2018, who created a similar checklist for the fields of ecology and evolution. Some of these points are adapted from the TTEE (Tools for Transparency in Ecology and Evolution) guidelines.

# Guidelines

These guidelines are presented in a way that reflects the typical structure for research articles. Specific sections are provided for articles that either involved quantitative research or the use of fossil specimens.

Some of the items in this checklist will not be applicable to different types of research paper.

## General 

* Have any relevant funding sources been disclosed

* Have any relevant conflicts of interest been disclosed

* Have any previous versions of this work been indicated (e.g., preprints or pre-registrations)

* Is the language used appropriate for a scientific publication

* Is the manuscript structure appropriate

### Figures

* Are any figures legible, relevant, and integrated into the text

### Tables

* Are any tables legible, relevant, and integrated into the text

### Supplementary files

* Are all supplementary files provided in non-proprietary formats and in a sustainable manner

* Are all supplementary files referenced within the main text

## Abstract

* Is the abstract concise, conveying the main research findings

* Are any key conclusions missing


## Introduction

* What type of study is this (e.g., replication, exploratory, meta-review)

* What is the primary purpose for conducting this study

* Is this part of a larger study, or a one-off

* What are the specific research questions addressed

* What is the scope of the study

* Is the published literature covered sufficiently

* Are any critical published articles missing

* Are any appropriate angles or context missing

* Is the history of the research sufficiently described


## Methods

* Are the methods reported in sufficient detail and clear enough to allow another researcher to gather the same data and run the identical analyses?

* When was this study started and for how long was its duration

### Data collection

* Are the supporting data included in the manuscript or in a relevant repository

* Are the data presented in a way that is consistent with the [FAIR principles](citation needed)

* When were the data collected

* Did the authors develop their analysis plan, including choices of variables, without looking at the data, for instance prior to gathering data or with a dummy data set?

* What were the conditions under which the data were collected or extracted

* Why was this study design selected

* Provide the precise details of data analysis, including information on software programs and packages, and annotated full code or set of commands.

* Are all information sources described, including accession dates

* Were data collected and vetted by all, some, or just one of the authors

* Were data obtained from previously published sources, or unpublished sources

* Is justification given for data that were ccollected but not including in the final analyses

* Were any assumptions, simplifications, or transformations to the data made

* Are any methods of weighting applied, and justifications for this given

* Are potential biases assessed and, if so, mitigated (e.g., publication bias, missing data, or selective reporting within studies)

* Complete and explicit database search terms including all logical connectors and operators sufficient to exactly duplicate search. Also, indicate all other sources of data (e.g. literature cited sections of published reviews, personal unpublished data, collaborative group project data available from cited sources or online).

* Are model selection criteria provided

### Involving fossil specimens

* If relevant, possible and allowable, deposit voucher specimens of the studied taxon/taxa in an appropriate curated collection 

* Are all analysed specimens available in a public and curated repository, and with accession numbers

* Were all relevant permits and permissions obtained to study specimens, and to publicise them

* Are all relevant contextual information provided, including:

  * Taxon name
  * Taxon level
  * Age of specimen, both stratigraphically and in Ma
  * Horizon of discovery, including bed, member, formation, and group
  * Lithology specimen discovered in, and environmental interpretations
  * Geographic location of specimen, including GPS co-ordinates and latitude and longitude
  * Discoverer of speciment, and date of discovery
  * Previous taxonomic diagnoses and information, including synonyms
  * Relevant type and referred specimens
  * Original, revised, or emended diagnoses
  * Taphonomy of specimen and mode of preservation
  * Methods of collection
  * Methods of preparation
  * Completeness of specimen and articulation
  * Any associated fossils

### Involving quantitative analysis

* What statistical tests were formed, and why were they chosen

* What is the total sample size

### Software environment

* Are all software and scripts identified, including their versions and availability

* Is the full model specification provided, including specific parameter choices and their justification

* Are all software and scripts available in a public repository and under an appropriate FOSS license

* Are code scripts appropriately annotated to faciliate easier understanding



## Results

* Are the results presented in a clear and coherent manner

* Are the results reported in a way that is supported by the data

* Is sufficient justification given for the erection of new taxa


### Involving quantitative analysis

* Are the full outcomes of statistical tests reported, including basic parameter estimates of central tendency (e.g., means), other basic estimates (e.g., regression and correlation coefficients), and variability (e.g., standard deviation) and associated estimates of uncertainty (e.g., confidence or credibility intervals)

* Are appropriate test statistics reported, including the degrees of freedom and p-values.

* For Bayesian analyses, this also should at a minimum include information on choice of priors and MCMC (Markov chain Monte Carlo) settings (e.g. burn-in, the number of iterations, and thinning intervals).

### Involving fossil specimens

* Is the morphology described in a systematic manner

* Are enough supporting images provided to illustrate relevant morphologies

* Are sufficient measurements provided


## Discussion

* Is the strength of the evidence evaluated and interpreted

* Are the sample sizes large enough to justify the authors’ conclusions? If presenting significance tests, how much power would this study have to detect statistically significant weak, moderate and strong effects?

* Are potential sources of bias, and their consequences, appropriately evaluated and interpreted

* Do the results contribute to the research field, irrespective of whether they are ‘negative’ or a replication study

* Are the new results placed into sufficient context of the relevant literature

* Are all arguments balanced

* Are the full implications of the new results discussed in sufficient detail


## Conclusions

* Are the conclusions supported by the results

* Are the conclusions consise and not over-embellished


# Discussion

Here, we provide no recommendations about how researchers should subjectively assess the 'impact' or 'quality' of a study.

This checklist is not designed as a replacement for critical thinking and the application of wider forms of expertise.

It is designed to provide a baseline standard for research such that minimum standards of reporting quality are met.

Individual reviewers can find this useful. Journals can also integrate these guidelines into their reviewer policies as part of ensuring that they are faciliating peer review to a sufficient quality standard.

* If we have appropriate standards for defining quality in peer review, then we no longer need to have secondary and redundant forms of post hoc evaluation based on things like journal brands.


References

Morey et al. 2018

TOP Guidelines